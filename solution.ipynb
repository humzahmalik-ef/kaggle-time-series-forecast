{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f36601",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e934517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=8)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, root_mean_squared_error, mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from dython.nominal import associations\n",
    "from dython.nominal import identify_nominal_columns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set option to display all columns (None means no limit to the number of columns displayed)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871df14",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e06575",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events_df=pd.read_csv(\"data/holidays_events.csv\")\n",
    "oil_df=pd.read_csv(\"data/oil.csv\")\n",
    "stores_df=pd.read_csv(\"data/stores.csv\")\n",
    "transactions_df=pd.read_csv(\"data/transactions.csv\")\n",
    "\n",
    "train_df=pd.read_csv(\"data/train.csv\", index_col=False)\n",
    "test_df=pd.read_csv(\"data/test.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_original=test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210314ed",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_nan(df):\n",
    "    # Count the number of NaN values in each column\n",
    "    nan_counts = df.isna().sum()\n",
    "\n",
    "    # Print the counts\n",
    "    print(f\"Number of rows in dataframe is: {len(df)}\")\n",
    "    print(nan_counts)\n",
    "    \n",
    "def count_nan_values(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame and returns a dictionary where the keys are the column names\n",
    "    and the values are the number of NaN values in each column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with column names as keys and the number of NaN values as values.\n",
    "    \"\"\"\n",
    "    nan_count_dict = df.isna().sum().to_dict()\n",
    "    return nan_count_dict\n",
    "\n",
    "\n",
    "def plot_two_cols(df, col_1, col_2):\n",
    "    \"\"\"\n",
    "    Plots date vs dcoilwtico from the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'date' and 'dcoilwtico' columns.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(df[col_1], df[col_2], marker='o', linestyle='-')\n",
    "    plt.xlabel(col_1)\n",
    "    plt.ylabel(col_2)\n",
    "    plt.title(f'{col_1} vs {col_2}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Dates\n",
    "def date_breakdown(df):\n",
    "    # Ensure the 'date' column is in datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    df['weekday'] = df['date'].dt.dayofweek\n",
    "    df['weekday_name'] = df['date'].dt.strftime('%A')\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['month_name'] = df['date'].dt.strftime('%B')\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5777152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in date columns\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
    "holidays_events_df['date'] = pd.to_datetime(holidays_events_df['date'])\n",
    "\n",
    "train_df = train_df.sort_values(by='date', ascending=True)\n",
    "test_df = test_df.sort_values(by='date', ascending=True)\n",
    "oil_df = oil_df.sort_values(by='date', ascending=True)\n",
    "holidays_events_df = holidays_events_df.sort_values(by='date', ascending=True)\n",
    "\n",
    "# Add train/test bool\n",
    "train_df['test'] = 0\n",
    "test_df['test'] = 1\n",
    "\n",
    "combined_df=pd.concat([train_df, test_df], ignore_index=True)\n",
    "combined_df=date_breakdown(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ec44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NaN values in Train DF: {count_nan_values(train_df)}\")\n",
    "print(f\"NaN values in Test DF: {count_nan_values(test_df)}\")\n",
    "print(f\"NaN values in Oil DF: {count_nan_values(oil_df)}\")\n",
    "print(f\"NaN values in Holidays DF: {count_nan_values(holidays_events_df)}\")\n",
    "print(f\"NaN values in Transactions DF: {count_nan_values(transactions_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check completeness of train\n",
    "min_date=train_df['date'].min()\n",
    "max_date=test_df['date'].max()\n",
    "\n",
    "print(f\"Minimum training date: {min_date}\")\n",
    "print(f\"Maximum training date: {max_date}\")\n",
    "\n",
    "# Get a list of all date ranges\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "def count_missing_dates_with_non_nan(expected_dates, oil_df):\n",
    "    # Convert expected_dates to a DataFrame\n",
    "    expected_df = pd.DataFrame({'date': expected_dates})\n",
    "    \n",
    "    # Merge with oil_df to find missing dates and NaN values\n",
    "    merged_df = expected_df.merge(oil_df, on='date', how='left')\n",
    "    \n",
    "    # Count dates that are either missing or have NaN values in dcoilwtico\n",
    "    missing_count = merged_df['dcoilwtico'].isna().sum()\n",
    "    \n",
    "    print(f\"Number of dates in expected_dates not in oil_df with a non-NaN value: {missing_count}\")\n",
    "    return missing_count\n",
    "\n",
    "def ensure_dates_in_df(oil_df, date_list):\n",
    "    # Ensure date column is in datetime format\n",
    "    oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
    "    date_list = pd.to_datetime(date_list)\n",
    "    \n",
    "    # Create a set of dates that need to be in the dataframe\n",
    "    date_set = set(date_list)\n",
    "    \n",
    "    # Get the existing dates in the dataframe\n",
    "    existing_dates = set(oil_df['date'])\n",
    "    \n",
    "    # Find the missing dates\n",
    "    missing_dates = date_set - existing_dates\n",
    "    \n",
    "    # Create a new dataframe with the missing dates and NaN values for dcoilwtico\n",
    "    missing_df = pd.DataFrame({'date': list(missing_dates), 'dcoilwtico': np.nan})\n",
    "    \n",
    "    # Concatenate the missing dates to the original dataframe\n",
    "    oil_df = pd.concat([oil_df, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Sort the dataframe by date\n",
    "    oil_df = oil_df.sort_values(by='date').reset_index(drop=True)\n",
    "    \n",
    "    return oil_df\n",
    "\n",
    "def fill_na_with_next_value(oil_df):\n",
    "    # Ensure the dataframe is sorted by date\n",
    "    oil_df = oil_df.sort_values(by='date').reset_index(drop=True)\n",
    "    \n",
    "    # Get the indices where dcoilwtico is NaN\n",
    "    nan_indices = oil_df[oil_df['dcoilwtico'].isna()].index\n",
    "    \n",
    "    for idx in nan_indices:\n",
    "        # Find the next non-NaN value\n",
    "        next_valid_index = oil_df.loc[idx:].dropna(subset=['dcoilwtico']).index[0]\n",
    "        next_valid_value = oil_df.at[next_valid_index, 'dcoilwtico']\n",
    "        \n",
    "        # Replace NaN with the next non-NaN value\n",
    "        oil_df.at[idx, 'dcoilwtico'] = next_valid_value\n",
    "    \n",
    "    return oil_df\n",
    "\n",
    "\n",
    "# Check all dates are present\n",
    "count_missing_dates_with_non_nan(expected_dates, oil_df)\n",
    "\n",
    "# Add missing dates\n",
    "oil_df=ensure_dates_in_df(oil_df, expected_dates)\n",
    "\n",
    "# Fill in NaNs\n",
    "oil_df=fill_na_with_next_value(oil_df)\n",
    "\n",
    "# Check all dates are present\n",
    "count_missing_dates_with_non_nan(expected_dates, oil_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "combined_df=combined_df.merge(stores_df, on='store_nbr', how='left')\n",
    "combined_df=combined_df.merge(oil_df, on='date', how='left')\n",
    "\n",
    "# Convert 'date' columns to datetime in both dataframes\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n",
    "\n",
    "# Now perform the merge\n",
    "combined_df = combined_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change type_x and type_y \n",
    "combined_df.rename(columns={'type': 'store_type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371acaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= combined_df[combined_df['test'] == 0]\n",
    "test_df= combined_df[combined_df['test'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4003",
   "metadata": {},
   "source": [
    "## Remove Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined\n",
    "combined_df.drop(columns=['transactions'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4818e",
   "metadata": {},
   "source": [
    "# Deal with Holiday NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a88737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values(df):\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"Column '{column}' has the following unique values:\")\n",
    "        print(unique_values)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "\n",
    "def fill_na_with_value(df: pd.DataFrame, columns: list, value: str = \"No Holiday\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill NaN values with a specified value in given columns of a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe to operate on.\n",
    "    columns (list): List of column names to fill NaN values in.\n",
    "    value (str): The value to fill NaN values with. Default is \"No Holiday\".\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with NaN values filled.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column].fillna(value, inplace=True)\n",
    "            \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6158876",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill=['holiday_type','locale','locale_name','description','transferred']\n",
    "\n",
    "combined_df=fill_na_with_value(combined_df, columns_to_fill, value= \"No Holiday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b54b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= combined_df[combined_df['test'] == 0]\n",
    "test_df= combined_df[combined_df['test'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f549c",
   "metadata": {},
   "source": [
    "## Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_rows(df):\n",
    "    \n",
    "    # Group by the specified columns and count the occurrences\n",
    "    grouped = df.groupby(['date', 'store_nbr', 'family']).size().reset_index(name='count')\n",
    "    \n",
    "    # Filter to only include rows with counts greater than 1\n",
    "    duplicates = grouped[grouped['count'] > 1]\n",
    "    \n",
    "    return duplicates\n",
    "    \n",
    "def check_duplicates_with_sales(df):\n",
    "    # Find duplicate rows based on 'date', 'store_nbr', and 'family'\n",
    "    duplicates = df[df.duplicated(subset=['date', 'store_nbr', 'family'], keep=False)]\n",
    "    \n",
    "    # Group by 'date', 'store_nbr', and 'family'\n",
    "    grouped_duplicates = duplicates.groupby(['date', 'store_nbr', 'family'])\n",
    "    \n",
    "    # Check for different sales values within each group\n",
    "    for _, group in grouped_duplicates:\n",
    "        if group['sales'].nunique() > 1:\n",
    "            print(list(group.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49720ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_keep_first(df):\n",
    "    # Remove duplicated rows based on 'date', 'store_nbr', and 'family', keeping the first instance\n",
    "    deduplicated_df = df.drop_duplicates(subset=['date', 'store_nbr', 'family'], keep='first')\n",
    "    \n",
    "    # Reset the index of the DataFrame\n",
    "    #deduplicated_df = deduplicated_df.reset_index(drop=True)\n",
    "    \n",
    "    return deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of combined_df pre duplication {len(combined_df)}\")\n",
    "combined_df=remove_duplicates_keep_first(combined_df)\n",
    "print(f\"Length of combined_df post duplication {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicate_rows(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= combined_df[combined_df['test'] == 0]\n",
    "test_df= combined_df[combined_df['test'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of training set {len(train_df)}\")\n",
    "print(f\"Length of test set {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46466a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ save combined_df -----\n",
    "combined_df_saved=combined_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1829a18",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167bb8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_column_frequency(df, col_name):\n",
    "    # Calculate the frequency of each value in the specified column\n",
    "    value_counts = df[col_name].value_counts()\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    value_counts.plot(kind='bar')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Label the y-axis and set the title\n",
    "    plt.ylabel(f\"Frequency\")\n",
    "    plt.title(f'Freq plot for column {col_name}')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "def plot_rows_per_day(df, date_column):\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Count the number of rows per day\n",
    "    counts_per_day = df.groupby(df[date_column].dt.date).size()\n",
    "    \n",
    "    # Plot the counts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    counts_per_day.plot(kind='line')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Rows')\n",
    "    plt.title('Number of Rows per Day')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_unique_stores_per_day(df, date_column, store_column):\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Group by date and count unique store numbers per day\n",
    "    unique_stores_per_day = df.groupby(df[date_column].dt.date)[store_column].nunique()\n",
    "    \n",
    "    # Plot the counts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    unique_stores_per_day.plot(kind='line')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Unique Stores')\n",
    "    plt.title('Number of Unique Stores per Day')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3adb5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Length of holiday df is: {len(holidays_events_df)}\")\n",
    "print(f\"Length of oil df is: {len(oil_df)}\")\n",
    "print(f\"Length of stores df is: {len(stores_df)}\")\n",
    "print(f\"Length of transactions df is: {len(transactions_df)}\")\n",
    "\n",
    "print(f\"Length of train is: {len(train_df)}\")\n",
    "print(f\"Length of test is: {len(test_df)}\")\n",
    "print(f\"Length of combined df is: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373f6c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Understanding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ab90f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_rows_per_day(combined_df, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdd3b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_unique_stores_per_day(combined_df, 'date', 'store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f1998",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'store_nbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a5f57",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4eb397",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'onpromotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab792d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'store_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcacf9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ecd6e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2354c7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'locale_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53652200",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'holiday_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c6408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_column_frequency(combined_df, 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13b0fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49412e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Relation to Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296c30a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plotting the frequency plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(train_df['sales'], bins=200, edgecolor='black')\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Plot of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6037656",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.histplot(np.log1p(train_df['sales']), kde=True, bins=50)\n",
    "plt.title('Log-Transformed Distribution of Daily Sales')\n",
    "plt.xlabel('Log(Sales + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a3800",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Ensure the date column is in datetime format\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Aggregate sales per day\n",
    "daily_sales = train_df.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Function to format the y-axis labels\n",
    "def millions(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    return '%1.0f' % x\n",
    "\n",
    "formatter = FuncFormatter(millions)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(daily_sales['date'], daily_sales['sales'])\n",
    "plt.title('Daily Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.grid(True)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9386e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the date column is in datetime format\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Aggregate average sales per store per day\n",
    "average_sales_per_store_daily = train_df.groupby(['date', 'store_nbr'])['sales'].mean().reset_index()\n",
    "daily_average_sales = average_sales_per_store_daily.groupby('date')['sales'].mean().reset_index()\n",
    "\n",
    "# Function to format the y-axis labels\n",
    "def millions(x, pos):\n",
    "    'The two args are the value and tick position'\n",
    "    return '%1.0f' % x\n",
    "\n",
    "formatter = FuncFormatter(millions)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(daily_average_sales['date'], daily_average_sales['sales'])\n",
    "plt.title('Daily Average Sales per Store')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.grid(True)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c927f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot of Sales vs Numerical Variables\n",
    "numerical_cols = ['onpromotion', 'dcoilwtico']\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    sns.scatterplot(x=col, y='sales', data=train_df)\n",
    "    plt.title(f'Sales vs {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Sales')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7c3c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Group by date and sum the sales\n",
    "daily_sales_sum = train_df.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Plotting the box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(daily_sales_sum['sales'], vert=False)\n",
    "plt.title('Box Plot of Daily Sales Sum')\n",
    "plt.xlabel('Sales Sum')\n",
    "plt.ylabel('Daily Sales')\n",
    "\n",
    "# Set the x-axis formatter to show absolute values\n",
    "plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a3077",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "categorical_features=identify_nominal_columns(train_df)\n",
    "print(f\"Initially recognized categorical Features: {categorical_features}\")\n",
    "\n",
    "categorical_cols=['store_nbr','family', 'weekday','weekday_name','month_name','quarter','city', 'state', 'store_type','cluster','holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "complete_correlation= associations(train_df, nominal_columns=categorical_cols, figsize=(22,22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb3f3d",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_complete_corr=complete_correlation['corr']\n",
    "df_complete_corr.dropna(axis=1, how='all').dropna(axis=0, how='all').style.background_gradient(cmap='coolwarm', axis=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6c5b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f8a4fd",
   "metadata": {},
   "source": [
    "# Pt. 1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a637a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df_saved.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_categorical(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].astype('category')\n",
    "    \n",
    "#     for column in df.columns:\n",
    "#         print(f\"Column: {column}, Type: {df[column].dtype}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continuous values\n",
    "scaler = StandardScaler()\n",
    "continous_features = ['dcoilwtico']\n",
    "combined_df[continous_features] = scaler.fit_transform(combined_df[continous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541fb896",
   "metadata": {},
   "source": [
    "# Pt. 2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035f40d",
   "metadata": {},
   "source": [
    "## Adding Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cd616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_features(df):\n",
    "    \n",
    "    # Create copy of df to work with\n",
    "    df_copy=df.copy()\n",
    "    \n",
    "    # Ensure the date column is in datetime format\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'])\n",
    "    \n",
    "    # Sort by date for each store_nbr and family combination\n",
    "    df_copy = df_copy.sort_values(by=['store_nbr', 'family', 'date'])\n",
    "    \n",
    "    # Set multi-index for easier group operations\n",
    "    df_copy.set_index(['store_nbr', 'family', 'date'], inplace=True)\n",
    "    \n",
    "    # Lagged features\n",
    "    df_copy['sales_lag_21'] = df_copy.groupby(['store_nbr', 'family'])['sales'].shift(21)\n",
    "    df_copy['sales_lag_28'] = df_copy.groupby(['store_nbr', 'family'])['sales'].shift(28)\n",
    "    \n",
    "    # Expanded mean excluding the current day\n",
    "    df_copy['sales_shifted'] = df_copy.groupby(['store_nbr', 'family'])['sales'].shift(1)\n",
    "    df_copy['sales_expanded_mean'] = df_copy.groupby(['store_nbr', 'family'])['sales_shifted'].expanding().mean().reset_index(level=[0,1], drop=True)\n",
    "    \n",
    "    # 1-month rolling mean excluding the current day (assuming '1 month' is 30 days)\n",
    "    df_copy['sales_rolling_mean_20'] = df_copy.groupby(['store_nbr', 'family'])['sales_shifted'].rolling(window=20, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "    df_copy['sales_rolling_mean_30'] = df_copy.groupby(['store_nbr', 'family'])['sales_shifted'].rolling(window=30, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "    df_copy['sales_rolling_mean_40'] = df_copy.groupby(['store_nbr', 'family'])['sales_shifted'].rolling(window=40, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "    \n",
    "    # Drop the shifted sales column used for calculations\n",
    "    df_copy.drop(columns=['sales_shifted'], inplace=True)\n",
    "    \n",
    "    # Reset the index\n",
    "    df_copy.reset_index(inplace=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    #df_copy = df_copy.sort_values(by='date', ascending=True)\n",
    "    \n",
    "    # Merge df_copy into df based on 'store_nbr', 'family', and 'date'\n",
    "    df_final = df.merge(df_copy[['store_nbr', 'family', 'date', 'sales_lag_21', 'sales_lag_28', \n",
    "                                 'sales_expanded_mean', 'sales_rolling_mean_20', 'sales_rolling_mean_30',\n",
    "                                'sales_rolling_mean_40']],\n",
    "                  on=['store_nbr', 'family', 'date'], \n",
    "                  how='left')\n",
    "    \n",
    "    df_final = df_final.sort_index()\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def compute_specific_day_sales_average(df):\n",
    "    # Sort the dataframe by 'date' to ensure proper chronological order\n",
    "    df = df.sort_values(by='date')\n",
    "    \n",
    "    # Create a new column for specific day sales average\n",
    "    df['specific_day_sales_average'] = np.nan\n",
    "    \n",
    "    store_count=1\n",
    "    \n",
    "    # Iterate over unique combinations of 'store_nbr' and 'family'\n",
    "    for store_nbr in df['store_nbr'].unique():\n",
    "        print(f\"On store iteration: {store_count}/{len(df['store_nbr'].unique())}\")\n",
    "        for family in df['family'].unique():\n",
    "            # Filter the dataframe for the current 'store_nbr' and 'family'\n",
    "            temp_df = df[(df['store_nbr'] == store_nbr) & (df['family'] == family)]\n",
    "            \n",
    "            # Iterate over each row in the filtered dataframe\n",
    "            for idx, row in temp_df.iterrows():\n",
    "                # Filter for rows with the same 'weekday_name' and before the current 'date'\n",
    "                past_days = temp_df[(temp_df['weekday_name'] == row['weekday_name']) & (temp_df['date'] < row['date'])]\n",
    "                \n",
    "                # Calculate the average sales for the past days\n",
    "                if not past_days.empty:\n",
    "                    average_sales = past_days['sales'].mean()\n",
    "                else:\n",
    "                    average_sales = np.nan\n",
    "                \n",
    "                # Update the 'specific_day_sales_average' for the current row\n",
    "                df.loc[idx, 'specific_day_sales_average'] = average_sales\n",
    "        \n",
    "        store_count+=1\n",
    "                \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdd1b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_df=add_lagged_features(combined_df)\n",
    "combined_df=compute_specific_day_sales_average(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df[combined_df['date']>'2013-01-28'] # Got to be greater than 28 days ago, due to lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8516718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length combined_df: {len(combined_df)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"NaN values in combined_df: {count_nan_values(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea19f47",
   "metadata": {},
   "source": [
    "# Pt. 3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df_time_saved=combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df_time_saved.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim dataset to 2016 onwards\n",
    "combined_df=combined_df[combined_df['year']>=2016]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018c932",
   "metadata": {},
   "source": [
    "# Pre Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to categorical type\n",
    "categorical_cols=['store_nbr','family', 'weekday','weekday_name','month_name','quarter','city', \n",
    "                 'state', 'store_type','cluster']\n",
    "\n",
    "\n",
    "# Convert categorical columns to categorical type\n",
    "combined_df=convert_columns_to_categorical(combined_df, categorical_cols)\n",
    "\n",
    "# Drop date\n",
    "combined_df = combined_df.drop(columns=['date'])\n",
    "\n",
    "train_df= combined_df[combined_df['test'] == 0]\n",
    "test_df= combined_df[combined_df['test'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ab699",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, variables):\n",
    "    \"\"\"\n",
    "    One hot encode the specified variables in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    variables (list): The list of column names to be one hot encoded.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with one hot encoded variables.\n",
    "    \"\"\"\n",
    "    df_encoded = pd.get_dummies(df, columns=variables, drop_first=False)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def label_encode(df, variables):\n",
    "    \"\"\"\n",
    "    Label encode the specified variables in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    variables (list): The list of column names to be label encoded.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with label encoded variables.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for var in variables:\n",
    "        if var in list(df_encoded.columns):\n",
    "            df_encoded[var] = le.fit_transform(df_encoded[var])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_sets(train_df, cols_to_exclude, categorical_cols, encode):\n",
    "    \n",
    "    train_df=train_df.drop(cols_to_exclude, axis=1)\n",
    "    \n",
    "    if encode==True:\n",
    "        \n",
    "        #train_df= one_hot_encode(train_df, categorical_cols)\n",
    "        train_df= label_encode(train_df, categorical_cols)\n",
    "        \n",
    "    # Separate features and target\n",
    "    X = train_df.drop(['sales'], axis=1)\n",
    "    y = train_df['sales']\n",
    "\n",
    "    # Splitting data into training and test sets based on rows\n",
    "    train_size = int(len(train_df) * 0.85)\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0cd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dataframe\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSLE', 'RMSE', 'MSE', 'MAE', 'MAE%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecca85a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9c0f3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df_log=train_df.copy()\n",
    "train_df_log['sales']=np.log(train_df['sales']+1)\n",
    "cols_to_exclude=['quarter', 'weekday_name', 'city', 'state', 'store_type', 'cluster']\n",
    "encode=True\n",
    "X_train, X_test, y_train, y_test= build_training_sets(train_df_log, cols_to_exclude, categorical_cols, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fb1b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back\n",
    "predictions = np.exp(predictions) - 1\n",
    "y_test = np.exp(y_test) - 1\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_test and predictions\n",
    "y_test_abs = abs(y_test)\n",
    "predictions_abs = abs(predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test_abs, predictions_abs))\n",
    "\n",
    "# Calculate the average of the target variable\n",
    "avg_target = np.mean(y_test)\n",
    "\n",
    "# Calculate MAE as a percentage of the average target variable value\n",
    "mae_percentage = (mae / avg_target) * 100\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "                            'RMSLE': [rmsle],\n",
    "                            'RMSE': [np.sqrt(mse)],\n",
    "                            'MSE': [mse],\n",
    "                            'MAE': [mae],\n",
    "                            'MAE%': [mae_percentage]}).round(2)\n",
    "\n",
    "results_df = pd.concat([results_df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce8c7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea23f7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dceb3a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df_log=train_df.copy()\n",
    "train_df_log['sales']=np.log(train_df['sales']+1)\n",
    "cols_to_exclude=['quarter', 'weekday_name', 'city', 'state', 'store_type', 'cluster']\n",
    "\n",
    "# Create dataset split\n",
    "encode=True\n",
    "X_train, X_test, y_train, y_test= build_training_sets(train_df_log, cols_to_exclude, categorical_cols, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ea6a4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ProgressRandomForestRegressor(RandomForestRegressor):\n",
    "    def fit(self, X, y):\n",
    "        # Wrapping the original fit method with tqdm to show progress\n",
    "        with tqdm(total=self.n_estimators) as pbar:\n",
    "            for i in range(self.n_estimators):\n",
    "                super().fit(X, y)\n",
    "                pbar.update(1)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc7fb6",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Regression Model\n",
    "model = ProgressRandomForestRegressor(n_estimators=20, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back\n",
    "predictions = np.exp(predictions) - 1\n",
    "y_test = np.exp(y_test) - 1\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_test and predictions\n",
    "y_test_abs = abs(y_test)\n",
    "predictions_abs = abs(predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test_abs, predictions_abs))\n",
    "\n",
    "# Calculate the average of the target variable\n",
    "avg_target = np.mean(y_test)\n",
    "\n",
    "# Calculate MAE as a percentage of the average target variable value\n",
    "mae_percentage = (mae / avg_target) * 100\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame({'Model': ['Random Forest'],\n",
    "                            'RMSLE': [rmsle],\n",
    "                            'RMSE': [np.sqrt(mse)],\n",
    "                            'MSE': [mse],\n",
    "                            'MAE': [mae],\n",
    "                            'MAE%': [mae_percentage]}).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378273ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a3905",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7fd91",
   "metadata": {},
   "source": [
    "## 3- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, X_train):\n",
    "    \"\"\"\n",
    "    Plots the feature importance of a model as a horizontal bar graph.\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained model with feature_importances_ attribute\n",
    "    X_train: Training data used for the model\n",
    "    \"\"\"\n",
    "    # Extract feature importances and feature names\n",
    "    feature_importances = model.feature_importances_\n",
    "    features = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for better manipulation\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "    # Sort the DataFrame by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Print col names by importance\n",
    "    print(importance_df['Feature'])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis() # To display the most important feature at the top\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a5d08",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9bbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sales column in training dataframe\n",
    "train_df_log=train_df.copy()\n",
    "train_df_log['sales']=np.log(train_df['sales']+1)\n",
    "encode=False\n",
    "cols_to_exclude=['id','quarter', 'weekday_name', 'city', 'state', 'store_type', 'cluster']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd434d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create dataset split\n",
    "X_train, X_test, y_train, y_test= build_training_sets(train_df_log, cols_to_exclude, categorical_cols, encode)\n",
    "\n",
    "# Initialize XGBoost model\n",
    "model = XGBRegressor(enable_categorical=True, eval_metric='rmsle')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back\n",
    "predictions = np.exp(predictions) - 1\n",
    "y_test = np.exp(y_test) - 1\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_test and predictions\n",
    "y_test_abs = abs(y_test)\n",
    "predictions_abs = abs(predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test_abs, predictions_abs))\n",
    "\n",
    "# Calculate the average of the target variable\n",
    "avg_target = np.mean(y_test)\n",
    "\n",
    "# Calculate MAE as a percentage of the average target variable value\n",
    "mae_percentage = (mae / avg_target) * 100\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame({'Model': ['XGBoost'],\n",
    "                            'RMSLE': [rmsle],\n",
    "                            'RMSE': [np.sqrt(mse)],\n",
    "                            'MSE': [mse],\n",
    "                            'MAE': [mae],\n",
    "                            'MAE%': [mae_percentage]}).round(2)\n",
    "\n",
    "results_df = pd.concat([results_df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(model, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on entire dataset\n",
    "X_train, X_test, y_train, y_test= build_training_sets(train_df_log, cols_to_exclude, categorical_cols, encode)\n",
    "model = XGBRegressor(enable_categorical=True, eval_metric='rmsle')\n",
    "model.fit(pd.concat([X_train, X_test], axis=0), pd.concat([y_train, y_test], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fe4bf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4) Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a480c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transform sales column in training dataframe\n",
    "train_df_log=train_df.copy()\n",
    "train_df_log['sales']=np.log(train_df['sales']+1)\n",
    "encode=False\n",
    "cols_to_exclude=['quarter', 'weekday_name', 'city', 'state', 'store_type', 'cluster']\n",
    "\n",
    "# Create dataset split\n",
    "X_train, X_test, y_train, y_test= build_training_sets(train_df_log, cols_to_exclude, categorical_cols, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3df33",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "catboost_categorical_cols = [col for col in categorical_cols if col in X_train.columns]\n",
    "model = CatBoostRegressor(cat_features=catboost_categorical_cols)  # Use GPU if available)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back\n",
    "predictions = np.exp(predictions) - 1\n",
    "y_test = np.exp(y_test) - 1\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_test and predictions\n",
    "y_test_abs = abs(y_test)\n",
    "predictions_abs = abs(predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test_abs, predictions_abs))\n",
    "\n",
    "# Calculate the average of the target variable\n",
    "avg_target = np.mean(y_test)\n",
    "\n",
    "# Calculate MAE as a percentage of the average target variable value\n",
    "mae_percentage = (mae / avg_target) * 100\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame({'Model': ['CatBoost'],\n",
    "                            'RMSLE': [rmsle],\n",
    "                            'RMSE': [np.sqrt(mse)],\n",
    "                            'MSE': [mse],\n",
    "                            'MAE': [mae],\n",
    "                            'MAE%': [mae_percentage]}).round(2)\n",
    "\n",
    "results_df = pd.concat([results_df, results], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02bcbf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Converting columns to standard notation\n",
    "# cols = ['RMSE', 'MSE', 'MAE', 'MAE%']\n",
    "# results_df[cols] = results_df[cols].applymap('{:.6f}'.format)\n",
    "# results_df[cols] = results_df[cols].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11627286",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3b695",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ba451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort train by id column\n",
    "test_df = test_df.sort_values(by='id', ascending=True)\n",
    "test_df.index = test_df['id']\n",
    "\n",
    "# Remove the index name\n",
    "test_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea30f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe with id column\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'sales': np.nan\n",
    "})\n",
    "\n",
    "# Build training set\n",
    "X = test_df.drop(['sales']+cols_to_exclude, axis=1)\n",
    "\n",
    "#Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Transform predictions back\n",
    "predictions = np.exp(predictions) - 1\n",
    "\n",
    "# Check predictions match number of entries\n",
    "if len(predictions) != len(submission_df):\n",
    "\n",
    "    print(\"Sizes dont match!\")\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Assigning the array to the 'sales' column\n",
    "    submission_df['sales'] = predictions\n",
    "\n",
    "    print(\"Column successfully replaced\")\n",
    "\n",
    "# Output submission file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
